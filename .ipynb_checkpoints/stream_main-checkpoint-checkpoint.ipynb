{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (2024.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from s3fs) (3.9.5)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2.13.0)\n",
      "Requirement already satisfied: fsspec==2024.5.0.* in /opt/conda/lib/python3.10/site-packages (from s3fs) (2024.5.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Requirement already satisfied: botocore<1.34.107,>=1.34.70 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.34.106)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.11.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.11)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'general_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeneral_functions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpyplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'general_functions'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "# import s3fs\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from general_functions.general_functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as pyplt\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import datetime\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('Data_Processing')\n",
    "logger.info('main.py Script started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_publisher_id_with_maximum_queries(spark, destination_path):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This function is used to transform the clean data, to get the top-5 publishers. \n",
    "        Parameters:\n",
    "        destination_path: This is the path of clean folder where data is being saved. \n",
    "        spark: spark object\n",
    "        returns: transformed pyspark-dataframe\n",
    "        \"\"\"\n",
    "        df  = spark.read.parquet(destination_path)\n",
    "        logger.info(f\"data loaded successfully from path:{destination_path}\")\n",
    "        df = df.withColumn(\"file_creation_date\", date_format('file_creation_date', \"yyyy-MM-dd\"))\n",
    "        window_spec = Window.partitionBy('publisher_id').orderBy(\"publisher_id\")\n",
    "        x = df.withColumn(\"total_count_of_click\", sum(col('total_clicks')).over(window_spec))\n",
    "        rank_spec = Window.partitionBy().orderBy(desc(col(\"total_count_of_click\")))\n",
    "        x = x.withColumn(\"rank\", dense_rank().over(rank_spec))\n",
    "        x = x.filter(x.rank<=5)\n",
    "        unique_publisher_id = list(set(x.select(\"publisher_id\").rdd.flatMap(lambda x: x).collect()))\n",
    "        v = df.filter(df.publisher_id.isin(unique_publisher_id))\n",
    "        v = v.groupBy(\"publisher_id\", \"file_creation_date\").agg(sum(col(\"total_clicks\")))\n",
    "        v = v.sort(\"publisher_id\", \"file_creation_date\", ascending=True)\n",
    "        v.show()\n",
    "        v.printSchema()\n",
    "        return v\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at generating_publisher_id_with_maximum_queries {e}\")\n",
    "\n",
    "def generatring_line_graph_for_top_5_publishers(df,destination_path):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This function is used to generate the line_graph and save it graph folder.\n",
    "        Parameters:\n",
    "        destination_path[String]: Path of graph folder to save the output line-graph.\n",
    "        df[Pyspark-dataframe]: Dataframe.\n",
    "        returns: None\n",
    "        \"\"\"\n",
    "        pyplt.rcParams[\"figure.figsize\"] = (50,15)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "        plt.set_loglevel('WARNING')\n",
    "\n",
    "        df = df.toPandas()\n",
    "        x_axis = sorted(df['file_creation_date'].drop_duplicates().to_list())\n",
    "        print(x_axis)\n",
    "        # fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\n",
    "        color_schema = ['r','y','g','c','k']\n",
    "        # unique_publisher_id = sorted(list(set(df.select(\"publisher_id\").rdd.flatMap(lambda x: x).collect())))\n",
    "        \n",
    "        unique_publisher_id = sorted(df['publisher_id'].drop_duplicates().to_list())\n",
    "        for i in range(len(unique_publisher_id)):\n",
    "            print(unique_publisher_id[i])\n",
    "\n",
    "            x_axis = np.array(sorted(df[df['publisher_id'] == unique_publisher_id[i]]['file_creation_date'].drop_duplicates().to_list()))\n",
    "\n",
    "            y_axis = np.array(df[df['publisher_id'] == unique_publisher_id[i]]['sum(total_clicks)'].to_list())//1000\n",
    "\n",
    "            y_smooth = gaussian_filter1d(y_axis, sigma=1)\n",
    "\n",
    "            # Plot smooth curve\n",
    "            plt.plot(x_axis, y_smooth,  f'x-{color_schema[i]}',label=unique_publisher_id[i], linewidth=4)\n",
    "\n",
    "\n",
    "        plt.xlabel('Date', size = 50, labelpad=38)\n",
    "        plt.ylabel('Clicks (x 1000)', size = 50, labelpad= 38)\n",
    "        plt.title('QPS', size = 50, pad = 6)\n",
    "        plt.xticks(fontsize=42,rotation=45,ha='right')\n",
    "        plt.yticks(fontsize=42)\n",
    "        specific_y_ticks = np.arange(0, 1200, 100)\n",
    "\n",
    "        plt.gca().set_yticks(specific_y_ticks)\n",
    "        plt.grid(visible = True,axis='y', which='both',color='k', linestyle='-', linewidth=0.6, in_layout=True)\n",
    "        plt.grid(visible = True,axis='x', which='both',color='k', linestyle='-', linewidth=0.6, in_layout=True)\n",
    "        plt.legend(prop={'size':50})\n",
    "        os.makedirs(\"graph\",exist_ok=True)\n",
    "        plt.savefig(f'{destination_path}/graph/line_graph.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at generatring_line_graph_for_top_5_publishers {e}\")\n",
    "\n",
    "def apply_transformations(spark,destination_path):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This function is used to transform raw data received in Landing folder and save it in clean folder as parquet format, with partitionBy date.\n",
    "        Parameters:\n",
    "        destination_path: This is the path of clean folder where data is being saved. \n",
    "        spark: spark object\n",
    "        returns: transformed pyspark-dataframe\n",
    "        \"\"\"\n",
    "        print(destination_path)\n",
    "        folder_path = []\n",
    "        # actual_path = destination_path.replace(\"\\\\\", \"/\")+'/Landing/click_log/2024/05/'\n",
    "        actual_path = destination_path\n",
    "        for outside in os.listdir(actual_path):\n",
    "            for inside in os.listdir(f'{actual_path}/{outside}'):\n",
    "                folder_path.append(f'{outside}/{inside}')\n",
    "\n",
    "\n",
    "        for path in folder_path:\n",
    "            logger.info(f\"currently working on folder:{path}\")\n",
    "            # df = spark.read.option(\"inferSchema\", True).option(\"mode\", \"PERMISSIVE\").json(f\"{actual_path}{path}/\")\n",
    "            spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "            df = (spark.readStream.option(\"cleanSource\",\"archive\").option(\"sourceArchiveDir\", \"archive_dir\").option(\"maxFilesPerTrigger\", 1).format(\"json\").load(\"C:/Users/Admin/Downloads/backup/root/stream_landing\"))\n",
    "            df = df.select('*', \"ip_geo.*\", \"query.*\").drop(\"query\", \"ip_geo\")\n",
    "            df = df.toDF(*get_unique_column_names(df.columns))\n",
    "\n",
    "            df = df.drop(*get_duplicate_column_names(df))\n",
    "\n",
    "            df = df.withColumn(\"real_filepath\", input_file_name())\n",
    "\n",
    "            df = df.withColumn(\"actual_file\" , split(df.real_filepath, '/',limit=-1))\n",
    "            df = df.withColumn(\"count_file\", size(df.actual_file))\n",
    "            df = df.withColumn(\"actual_file\" , df.actual_file[col(\"count_file\")-1]).drop(\"count_file\")\n",
    "            df = df.withColumn(\"file_creation_date\", get_file_generation_date_udf(col(\"actual_file\")))\n",
    "            df = df.withColumn(\"file_creation_date\", date_format(to_timestamp(\"file_creation_date\", \"yyyy-MM-dd HH-mm\"), \"yyyy-MM-dd HH:mm\"))\n",
    "            publisher_id  = get_publisher_id_column_name(df)\n",
    "            df = df.na.fill(\"null\")\n",
    "            # print(\"this is the column structure\", df.columns)\n",
    "            df = df.withColumnRenamed(publisher_id, \"publisher_id\")\n",
    "            df = df.select(\"publisher_id\", \"file_creation_date\", \"actual_file\")\n",
    "            df = df.withColumn(\"publisher_id\", when(length(col(\"publisher_id\")) > 6, regexp_extract(col(\"publisher_id\"), \"^(va-\\d{3})|^(VA-\\d{3})\",0)).otherwise(col(\"publisher_id\")))\n",
    "            \n",
    "            \n",
    "            df = df.groupBy(\"publisher_id\", \"file_creation_date\", \"actual_file\").agg(count(\"publisher_id\").alias(\"total_clicks\"))\n",
    "            # # print(\"this is the window function count\", df.count())\n",
    "            df = df.withColumn(\"date\", split(col(\"file_creation_date\"), \" \").getItem(0))\n",
    "            df = df.withColumn(\"date\", to_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "            df = df.withColumn(\"path\", lit(path))\n",
    "            df.printSchema()\n",
    "\n",
    "            # df.write.partitionBy(\"date\").mode(\"append\").format(\"parquet\").save(str(os.getcwd()).replace(\"\\\\\", \"/\")+f'/clean1')\n",
    "            streaming_df.writeStream.format(\"console\").option(\"checkpointLocation\", \"C:/Users/Admin/Downloads/backup/checkpoint/\").outputMode(\"append\").start().awaitTermination()\n",
    "            logger.info(f\"successfully saved data of {path} with partiton column date\")\n",
    "        return str(os.getcwd()).replace(\"\\\\\", \"/\")+f'/clean1'\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at apply_transformations {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     if __name__ == \"__main__\":\n",
    "\n",
    "#         #public s3 path. \n",
    "#         s3_path = 'datasci-assignment/click_log/2024/05/'\n",
    "\n",
    "#         #Create S3 object to read from public S3-bucket.\n",
    "#         s3 = s3fs.S3FileSystem(anon =  True)\n",
    "\n",
    "#         #getting currect working directory to save the files in landing location. \n",
    "#         currect_working_directory = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "#         logger.info(f\"ingestion of data started from s3-path {s3_path}\")\n",
    "\n",
    "#         #Start INgesting data in Landing folder.\n",
    "#         destination_path  = ingest_data_from_s3(currect_working_directory, s3, s3_path)\n",
    "#         logger.info(f\"ingestion of data Completed successfully at location {destination_path}\")\n",
    "\n",
    "\n",
    "#         spark = SparkSession.builder.master(\"local[*]\").appName(\"Batch_procecssing_pipeline_from_s3\").config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\").getOrCreate()\n",
    "#         logger.info(f\"SparkSession Created Successfully\")\n",
    "        \n",
    "#         logger.info(f\"apply_transformations function started successfully reading data from location : {destination_path}\")\n",
    "#         destination_path = apply_transformations(spark,destination_path)\n",
    "#         logger.info(f\"apply_transformations function completed saved parquet at location: {destination_path}\")\n",
    "#         df = generating_publisher_id_with_maximum_queries(spark, destination_path)\n",
    "#         logger.info(\"generating_publisher_id_with_maximum_queries function runned successfully\")\n",
    "#         df.coalesce(1).write.mode(\"overwrite\").csv(\"top_5_publishers_id_data\")\n",
    "#         logger.info(\"top-5 publishers_id saved in csv file\")\n",
    "#         generatring_line_graph_for_top_5_publishers(df, os.getcwd())\n",
    "#         logger.info(f\"generatring_line_graph_for_top_5_publishers function completed saved parquet at location: {destination_path}\")\n",
    "            \n",
    "# except Exception as e:\n",
    "#         logger.info(f\"Error has been encountered at main {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# destination_path = apply_transformations(spark,destination_path)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# while True:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     print(\"yes\")\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241m.\u001b[39minfo(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[0;32m      7\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch_procecssing_pipeline_from_s3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.legacy.timeParserPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEGACY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      8\u001b[0m spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.streaming.schemaInference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# destination_path = apply_transformations(spark,destination_path)\n",
    "# while True:\n",
    "#     print(\"yes\")\n",
    "\n",
    "logger.info(os.getcwd())\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Batch_procecssing_pipeline_from_s3\").config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "df = (spark.readStream.option(\"cleanSource\",\"archive\").option(\"sourceArchiveDir\", \"./archived/here/\").option(\"maxFilesPerTrigger\", 1).format(\"json\").load(f\"{os.getcwd()}/landing/\"))\n",
    "df.printSchema()\n",
    "df = df.select('*', \"ip_geo.*\", \"query.*\").drop(\"query\", \"ip_geo\")\n",
    "df = df.toDF(*get_unique_column_names(df.columns))\n",
    "\n",
    "df = df.drop(*get_duplicate_column_names(df))\n",
    "\n",
    "df = df.withColumn(\"real_filepath\", input_file_name())\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"click_time\" ,col(\"click_time_1\").cast(\"timestamp\"))\n",
    "df.printSchema()\n",
    "# df.show()\n",
    "df = df.withColumn(\"actual_file\" , split(df.real_filepath, '/',limit=-1))\n",
    "df = df.withColumn(\"count_file\", size(df.actual_file))\n",
    "df = df.withColumn(\"actual_file\" , df.actual_file[col(\"count_file\")-1]).drop(\"count_file\")\n",
    "df = df.withColumn(\"file_creation_date\", get_file_generation_date_udf(col(\"actual_file\")))\n",
    "df = df.withColumn(\"file_creation_date\", to_timestamp(\"file_creation_date\", \"yyyy-MM-dd HH-mm\"))\n",
    "publisher_id  = get_publisher_id_column_name(df)\n",
    "df = df.na.fill(\"null\")\n",
    "# df.printSchema()\n",
    "# # print(\"this is the column structure\", df.columns)\n",
    "df = df.withColumnRenamed(publisher_id, \"publisher_id\")\n",
    "df = df.select(\"publisher_id\", \"file_creation_date\", \"actual_file\",\"click_time\")\n",
    "df = df.withColumn(\"publisher_id\", when(length(col(\"publisher_id\")) > 6, regexp_extract(col(\"publisher_id\"), \"^(va-\\d{3})|^(VA-\\d{3})\",0)).otherwise(col(\"publisher_id\")))\n",
    "\n",
    "# df.printSchema()\n",
    "df = df.withWatermark(\"click_time\", \"10 minutes\").groupBy(window(\"click_time\", \"10 minutes\"),\"publisher_id\", \"file_creation_date\", \"actual_file\").agg(count(\"publisher_id\").alias(\"total_clicks\"))\n",
    "# print(\"this is the window function count\", df.count())\n",
    "df = df.withColumn(\"date\", split(col(\"file_creation_date\"), \" \").getItem(0))\n",
    "df = df.withColumn(\"date\", to_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "# df = df.withColumn(\"path\", lit(path))\n",
    "# df.printSchema()\n",
    "\n",
    "# df.write.partitionBy(\"date\").mode(\"append\").format(\"parquet\").save(str(os.getcwd()).replace(\"\\\\\", \"/\")+f'/clean1')\n",
    "df.writeStream.format(\"console\").option(\"checkpointLocation\", f\"{os.getcwd()}/checkpoint/\").trigger(processingTime=\"30 seconds\").outputMode(\"append\").start().awaitTermination()\n",
    "# .option(\"path\", f\"{os.getcwd()}/output/\")\n",
    "# .option(\"path\", f\"{os.getcwd()}/output/\")\n",
    "# logger.info(f\"successfully saved data of {path} with partiton column date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- campaign_id: string (nullable = true)\n",
      " |-- click_time: long (nullable = true)\n",
      " |-- creative_id: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- gaid: string (nullable = true)\n",
      " |-- idfa: string (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- ip_geo: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- city_name: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- lat: string (nullable = true)\n",
      " |    |-- lon: string (nullable = true)\n",
      " |    |-- metro_code: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- zip: string (nullable = true)\n",
      " |-- isGlobalIpFlag: long (nullable = true)\n",
      " |-- p_click_id: string (nullable = true)\n",
      " |-- publisher_id: string (nullable = true)\n",
      " |-- query: struct (nullable = true)\n",
      " |    |-- campaign_id: string (nullable = true)\n",
      " |    |-- click_id: string (nullable = true)\n",
      " |    |-- creative_id: string (nullable = true)\n",
      " |    |-- gaid: string (nullable = true)\n",
      " |    |-- idfa: string (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- os_version: string (nullable = true)\n",
      " |    |-- publisher_id: string (nullable = true)\n",
      " |    |-- sub_publisher_id: string (nullable = true)\n",
      " |    |-- user_agent: string (nullable = true)\n",
      " |-- req: string (nullable = true)\n",
      " |-- sub_publisher_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Batch_procecssing_pipeline_from_s3\").config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "df = (spark.readStream.option(\"cleanSource\",\"archive\").option(\"sourceArchiveDir\", \"./archived/here/\").option(\"maxFilesPerTrigger\", 1).format(\"json\").load(\"C:/Users/Admin/Downloads/stream_task/stream_landing/\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
