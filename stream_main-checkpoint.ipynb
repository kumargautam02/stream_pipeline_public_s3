{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (2024.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from s3fs) (3.9.5)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2.13.0)\n",
      "Requirement already satisfied: fsspec==2024.5.0.* in /opt/conda/lib/python3.10/site-packages (from s3fs) (2024.5.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Requirement already satisfied: botocore<1.34.107,>=1.34.70 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.34.106)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.11.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.11)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.107,>=1.34.70->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 13:47:42,145 - main.py Script started\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "# import s3fs\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# from general_functions.general_functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as pyplt\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "import datetime\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger('Data_Processing')\n",
    "logger.info('main.py Script started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Processing.logger_configuration.logger_config\n",
    "import os\n",
    "from Processing.logger_config import *\n",
    "from Processing.stream_processing_script import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "def ingest_data_from_s3(current_working, s3, s3_path):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This function is used to Ingest data from Public S3 bucket site using s3fs library and store inside Landing Folder of Current working Directory. \n",
    "        Parameters:\n",
    "        current_working: receive the current working directory to save data ex:- current_working_directory/Landing\n",
    "        s3: S3 object for Public s3_bucket.\n",
    "        s3_path: s3 bucket path from which we have to ingest data. \n",
    "\n",
    "        returns: None\n",
    "        \"\"\"\n",
    "\n",
    "        # current_working = os.getcwd()\n",
    "        # s3.get(f'{s3_path}', f'{current_working}/Landing1/',recursive=True, maxdepth=None)\n",
    "        return f'{current_working}/Landing1/'\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at ingest_data_from_s3 {e}\")\n",
    "\n",
    "\n",
    "def get_file_generation_date(column):\n",
    "    \"\"\"\n",
    "    This function is used to extract the date_column from file_name.\n",
    "    Parameters:\n",
    "    column[pyspark dataframe column]: column with the file_name.\n",
    "\n",
    "    returns: the date of file generation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pattern_for_date = \"(2024-05-\\d{2})\"\n",
    "        pattern_for_whole = \"(2024-05-\\d{2}-\\d{2}-\\d{2}-\\d{2})\"\n",
    "        date_value = re.search(pattern_for_date, column).group(0)\n",
    "        whole_value = re.search(pattern_for_whole, column).group(0)\n",
    "        time_value = whole_value.replace(f'{date_value}-', \"\")\n",
    "\n",
    "        return f\"{date_value} {time_value}\"\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at get_file_generation_date {e}\")\n",
    "\n",
    "\n",
    "\n",
    "get_file_generation_date_udf  = udf(lambda column: get_file_generation_date(column),StringType())\n",
    "\n",
    "\n",
    "def get_unique_column_names(column_names):\n",
    "    \"\"\"\n",
    "    This function is used to give unique names to spark columns based on index of the columns.\n",
    "    Parameters:\n",
    "    column[python list]: Python list with the column_names\n",
    "\n",
    "    returns: [python list with unique names]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(column_names)\n",
    "        for i in range(len(column_names)):\n",
    "            if column_names[i].strip() != \"file_creation_date\":\n",
    "                column_names[i] = column_names[i] + f\"_{i}\" \n",
    "\n",
    "        return column_names\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at get_unique_column_names {e}\")\n",
    "    \n",
    "\n",
    "def get_duplicate_column_names(df):\n",
    "    \"\"\"\n",
    "    This function is used to get the duplciate columns from the dataframe.\n",
    "    Parameters:\n",
    "    df[pyspark dataframe column]: pyspark dataframe\n",
    "\n",
    "    returns: [Python list] list of duplicate column names present in the dataframe.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        duplicate_columns = []\n",
    "        original_columns = []\n",
    "        for column in df.columns:\n",
    "            # print(column)\n",
    "            if column.rsplit(\"_\", 1)[0] not in original_columns:\n",
    "                original_columns.append(column.rsplit(\"_\", 1)[0])\n",
    "            else:\n",
    "                duplicate_columns.append(column)\n",
    "        return duplicate_columns\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at get_duplicate_column_names {e}\")\n",
    "\n",
    "\n",
    "def get_publisher_id_column_name(df):\n",
    "    \"\"\"\n",
    "    This function is used to get the publisher_id column name.\n",
    "    Parameters:\n",
    "    column[pyspark dataframe column]: dataframe.\n",
    "\n",
    "    returns: [python string] column name of publisher_id\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(\"yes\")\n",
    "        for column in df.columns:\n",
    "            # print(column)\n",
    "            if column.rsplit(\"_\", 1)[0] == 'publisher_id':\n",
    "                # print(column)\n",
    "                \n",
    "                return column\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at get_publisher_id_column_name {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_transformations(spark,destination_path):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This function is used to transform raw data received in Landing folder and save it in clean folder as parquet format, with partitionBy date.\n",
    "        Parameters:\n",
    "        destination_path: This is the path of clean folder where data is being saved. \n",
    "        spark: spark object\n",
    "        returns: transformed pyspark-dataframe\n",
    "        \"\"\"\n",
    "        logger = get_logger()\n",
    "        logger.info('stream_processing_Script started')\n",
    "        df = (spark.readStream.option(\"cleanSource\",\"archive\")\n",
    "              .option(\"sourceArchiveDir\", f\"{destination_path}/Files_done/here/\")\n",
    "              .option(\"maxFilesPerTrigger\", 1).format(\"json\").load(f\"{destination_path}/landing\"))\n",
    "        # df.printSchema()\n",
    "        df = df.select('*', \"ip_geo.*\", \"query.*\").drop(\"query\", \"ip_geo\")\n",
    "        df = df.toDF(*get_unique_column_names(df.columns))\n",
    "\n",
    "        df = df.drop(*get_duplicate_column_names(df))\n",
    "        df = df.toDF(*[column.rsplit(\"_\",1)[0] for column in df.columns])\n",
    "\n",
    "        df = df.withColumn(\"real_filepath\", input_file_name())\n",
    "        # df.printSchema()\n",
    "        df = df.withColumn(\"click_time\" ,col(\"click_time\").cast(\"timestamp\"))\n",
    "        # df.printSchema()\n",
    "        # df.show()\n",
    "        df = df.withColumn(\"actual_file\" , split(df.real_filepath, '/',limit=-1))\n",
    "        df = df.withColumn(\"count_file\", size(df.actual_file))\n",
    "        df = df.withColumn(\"actual_file\" , df.actual_file[col(\"count_file\")-1]).drop(\"count_file\")\n",
    "        df = df.withColumn(\"file_creation_date\", get_file_generation_date_udf(col(\"actual_file\")))\n",
    "        df = df.withColumn(\"file_creation_date\", to_timestamp(\"file_creation_date\", \"yyyy-MM-dd HH-mm\"))\n",
    "        publisher_id  = get_publisher_id_column_name(df)\n",
    "        df = df.na.fill(\"null\")\n",
    "        # df.printSchema()\n",
    "        # # print(\"this is the column structure\", df.columns)\n",
    "        df = df.withColumnRenamed(publisher_id, \"publisher_id\")\n",
    "        df = df.select(\"publisher_id\", \"file_creation_date\", \"actual_file\",\"click_time\")\n",
    "        df = df.withColumn(\"publisher_id\", when(length(col(\"publisher_id\")) > 6, regexp_extract(col(\"publisher_id\"), \"^(va-\\d{3})|^(VA-\\d{3})\",0)).otherwise(col(\"publisher_id\")))\n",
    "\n",
    "        # df.printSchema()\n",
    "        df = df.withWatermark(\"click_time\", \"10 minutes\").groupBy(window(\"click_time\", \"10 minutes\"),\"publisher_id\", \"file_creation_date\", \"actual_file\").agg(count(\"publisher_id\").alias(\"total_clicks\"))\n",
    "        # print(\"this is the window function count\", df.count())\n",
    "        df = df.withColumn(\"date\", split(col(\"file_creation_date\"), \" \").getItem(0))\n",
    "        df = df.withColumn(\"date\", to_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "        # df = df.withColumn(\"path\", lit(path))\n",
    "        # df.printSchema()\n",
    "\n",
    "        # df.write.partitionBy(\"date\").mode(\"append\").format(\"parquet\").save(str(os.getcwd()).replace(\"\\\\\", \"/\")+f'/clean1')\n",
    "        # df.writeStream.format(\"console\").trigger(processingTime=\"30 seconds\").start().awaitTermination()\n",
    "        df.writeStream.format(\"console\").option(\"checkpointLocation\", f\"{destination_path}/checkpoint/\").trigger(processingTime=\"30 seconds\").outputMode(\"append\").start().awaitTermination()\n",
    "        # .option(\"path\", f\"{destination_path}/output/\")\n",
    "        # .option(\"path\", f\"{os.getcwd()}/output/\")\n",
    "        return f\"{destination_path}/output/\"\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at apply_transformations {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# try:\n",
    "#     if __name__ == \"__main__\":\n",
    "#         logger = get_logger()\n",
    "\n",
    "#         currect_working_directory = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "#         logger.info(f\"current working directory: {currect_working_directory}\")\n",
    "\n",
    "\n",
    "\n",
    "#         spark = SparkSession.builder.master(\"local[*]\").appName(\"stream_procecssing_pipeline_from_s3\").config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\").getOrCreate()\n",
    "#         logger.info(f\"SparkSession Created Successfully\")\n",
    "#         spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "\n",
    "        \n",
    "#         logger.info(f\"apply_transformations function started successfully reading data from location : {currect_working_directory}/landing\")\n",
    "#         destination_path = apply_transformations(spark,f\"{currect_working_directory}\")\n",
    "\n",
    "        # logger.info(f\"apply_transformations function completed saved parquet at location: {destination_path}\")\n",
    "        # df = generating_publisher_id_with_maximum_queries(spark, destination_path)\n",
    "        # logger.info(\"generating_publisher_id_with_maximum_queries function runned successfully\")\n",
    "        # df.coalesce(1).write.mode(\"overwrite\").csv(\"top_5_publishers_id_data\")\n",
    "        # logger.info(\"top-5 publishers_id saved in csv file\")\n",
    "        # generatring_line_graph_for_top_5_publishers(df, os.getcwd())\n",
    "        # logger.info(f\"generatring_line_graph_for_top_5_publishers function completed saved parquet at location: {destination_path}\")\n",
    "            \n",
    "# except Exception as e:\n",
    "#         logger.info(f\"Error has been encountered at main {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generating_publisher_id_with_maximum_queries(spark, destination_path):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This function is used to transform the clean data, to get the top-5 publishers. \n",
    "        Parameters:\n",
    "        destination_path: This is the path of clean folder where data is being saved. \n",
    "        spark: spark object\n",
    "        returns: transformed pyspark-dataframe\n",
    "        \"\"\"\n",
    "        df  = spark.read.parquet(destination_path)\n",
    "        logger.info(f\"data loaded successfully from path:{destination_path}\")\n",
    "        df = df.withColumn(\"file_creation_date\", date_format('file_creation_date', \"yyyy-MM-dd\"))\n",
    "        window_spec = Window.partitionBy('publisher_id').orderBy(\"publisher_id\")\n",
    "        x = df.withColumn(\"total_count_of_click\", sum(col('total_clicks')).over(window_spec))\n",
    "        rank_spec = Window.partitionBy().orderBy(desc(col(\"total_count_of_click\")))\n",
    "        x = x.withColumn(\"rank\", dense_rank().over(rank_spec))\n",
    "        x = x.filter(x.rank<=5)\n",
    "        unique_publisher_id = list(set(x.select(\"publisher_id\").rdd.flatMap(lambda x: x).collect()))\n",
    "        v = df.filter(df.publisher_id.isin(unique_publisher_id))\n",
    "        v = v.groupBy(\"publisher_id\", \"file_creation_date\").agg(sum(col(\"total_clicks\")))\n",
    "        v = v.sort(\"publisher_id\", \"file_creation_date\", ascending=True)\n",
    "        v.show()\n",
    "        v.printSchema()\n",
    "        return v\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at generating_publisher_id_with_maximum_queries {e}\")\n",
    "\n",
    "def generatring_line_graph_for_top_5_publishers(df,destination_path):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This function is used to generate the line_graph and save it graph folder.\n",
    "        Parameters:\n",
    "        destination_path[String]: Path of graph folder to save the output line-graph.\n",
    "        df[Pyspark-dataframe]: Dataframe.\n",
    "        returns: None\n",
    "        \"\"\"\n",
    "        pyplt.rcParams[\"figure.figsize\"] = (50,15)\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "        plt.set_loglevel('WARNING')\n",
    "\n",
    "        df = df.toPandas()\n",
    "        x_axis = sorted(df['file_creation_date'].drop_duplicates().to_list())\n",
    "        print(x_axis)\n",
    "        # fig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\n",
    "        color_schema = ['r','y','g','c','k']\n",
    "        # unique_publisher_id = sorted(list(set(df.select(\"publisher_id\").rdd.flatMap(lambda x: x).collect())))\n",
    "        \n",
    "        unique_publisher_id = sorted(df['publisher_id'].drop_duplicates().to_list())\n",
    "        for i in range(len(unique_publisher_id)):\n",
    "            print(unique_publisher_id[i])\n",
    "\n",
    "            x_axis = np.array(sorted(df[df['publisher_id'] == unique_publisher_id[i]]['file_creation_date'].drop_duplicates().to_list()))\n",
    "\n",
    "            y_axis = np.array(df[df['publisher_id'] == unique_publisher_id[i]]['sum(total_clicks)'].to_list())//1000\n",
    "\n",
    "            y_smooth = gaussian_filter1d(y_axis, sigma=1)\n",
    "\n",
    "            # Plot smooth curve\n",
    "            plt.plot(x_axis, y_smooth,  f'x-{color_schema[i]}',label=unique_publisher_id[i], linewidth=4)\n",
    "\n",
    "\n",
    "        plt.xlabel('Date', size = 50, labelpad=38)\n",
    "        plt.ylabel('Clicks (x 1000)', size = 50, labelpad= 38)\n",
    "        plt.title('QPS', size = 50, pad = 6)\n",
    "        plt.xticks(fontsize=42,rotation=45,ha='right')\n",
    "        plt.yticks(fontsize=42)\n",
    "        specific_y_ticks = np.arange(0, 1200, 100)\n",
    "\n",
    "        plt.gca().set_yticks(specific_y_ticks)\n",
    "        plt.grid(visible = True,axis='y', which='both',color='k', linestyle='-', linewidth=0.6, in_layout=True)\n",
    "        plt.grid(visible = True,axis='x', which='both',color='k', linestyle='-', linewidth=0.6, in_layout=True)\n",
    "        plt.legend(prop={'size':50})\n",
    "        os.makedirs(\"graph\",exist_ok=True)\n",
    "        plt.savefig(f'{destination_path}/graph/line_graph.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at generatring_line_graph_for_top_5_publishers {e}\")\n",
    "\n",
    "def apply_transformations(spark,destination_path):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        This function is used to transform raw data received in Landing folder and save it in clean folder as parquet format, with partitionBy date.\n",
    "        Parameters:\n",
    "        destination_path: This is the path of clean folder where data is being saved. \n",
    "        spark: spark object\n",
    "        returns: transformed pyspark-dataframe\n",
    "        \"\"\"\n",
    "        print(destination_path)\n",
    "        folder_path = []\n",
    "        # actual_path = destination_path.replace(\"\\\\\", \"/\")+'/Landing/click_log/2024/05/'\n",
    "        actual_path = destination_path\n",
    "        for outside in os.listdir(actual_path):\n",
    "            for inside in os.listdir(f'{actual_path}/{outside}'):\n",
    "                folder_path.append(f'{outside}/{inside}')\n",
    "\n",
    "\n",
    "        for path in folder_path:\n",
    "            logger.info(f\"currently working on folder:{path}\")\n",
    "            # df = spark.read.option(\"inferSchema\", True).option(\"mode\", \"PERMISSIVE\").json(f\"{actual_path}{path}/\")\n",
    "            spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "            df = (spark.readStream.option(\"cleanSource\",\"archive\").option(\"sourceArchiveDir\", \"archive_dir\").option(\"maxFilesPerTrigger\", 1).format(\"json\").load(\"C:/Users/Admin/Downloads/backup/root/stream_landing\"))\n",
    "            df = df.select('*', \"ip_geo.*\", \"query.*\").drop(\"query\", \"ip_geo\")\n",
    "            df = df.toDF(*get_unique_column_names(df.columns))\n",
    "\n",
    "            df = df.drop(*get_duplicate_column_names(df))\n",
    "\n",
    "            df = df.withColumn(\"real_filepath\", input_file_name())\n",
    "\n",
    "            df = df.withColumn(\"actual_file\" , split(df.real_filepath, '/',limit=-1))\n",
    "            df = df.withColumn(\"count_file\", size(df.actual_file))\n",
    "            df = df.withColumn(\"actual_file\" , df.actual_file[col(\"count_file\")-1]).drop(\"count_file\")\n",
    "            df = df.withColumn(\"file_creation_date\", get_file_generation_date_udf(col(\"actual_file\")))\n",
    "            df = df.withColumn(\"file_creation_date\", date_format(to_timestamp(\"file_creation_date\", \"yyyy-MM-dd HH-mm\"), \"yyyy-MM-dd HH:mm\"))\n",
    "            publisher_id  = get_publisher_id_column_name(df)\n",
    "            df = df.na.fill(\"null\")\n",
    "            # print(\"this is the column structure\", df.columns)\n",
    "            df = df.withColumnRenamed(publisher_id, \"publisher_id\")\n",
    "            df = df.select(\"publisher_id\", \"file_creation_date\", \"actual_file\")\n",
    "            df = df.withColumn(\"publisher_id\", when(length(col(\"publisher_id\")) > 6, regexp_extract(col(\"publisher_id\"), \"^(va-\\d{3})|^(VA-\\d{3})\",0)).otherwise(col(\"publisher_id\")))\n",
    "            \n",
    "            \n",
    "            df = df.groupBy(\"publisher_id\", \"file_creation_date\", \"actual_file\").agg(count(\"publisher_id\").alias(\"total_clicks\"))\n",
    "            # # print(\"this is the window function count\", df.count())\n",
    "            df = df.withColumn(\"date\", split(col(\"file_creation_date\"), \" \").getItem(0))\n",
    "            df = df.withColumn(\"date\", to_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "            df = df.withColumn(\"path\", lit(path))\n",
    "            df.printSchema()\n",
    "\n",
    "            # df.write.partitionBy(\"date\").mode(\"append\").format(\"parquet\").save(str(os.getcwd()).replace(\"\\\\\", \"/\")+f'/clean1')\n",
    "            streaming_df.writeStream.format(\"console\").option(\"checkpointLocation\", \"C:/Users/Admin/Downloads/backup/checkpoint/\").outputMode(\"append\").start().awaitTermination()\n",
    "            logger.info(f\"successfully saved data of {path} with partiton column date\")\n",
    "        return str(os.getcwd()).replace(\"\\\\\", \"/\")+f'/clean1'\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Error has been encountered at apply_transformations {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     if __name__ == \"__main__\":\n",
    "\n",
    "#         #public s3 path. \n",
    "#         s3_path = 'datasci-assignment/click_log/2024/05/'\n",
    "\n",
    "#         #Create S3 object to read from public S3-bucket.\n",
    "#         s3 = s3fs.S3FileSystem(anon =  True)\n",
    "\n",
    "#         #getting currect working directory to save the files in landing location. \n",
    "#         currect_working_directory = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "#         logger.info(f\"ingestion of data started from s3-path {s3_path}\")\n",
    "\n",
    "#         #Start INgesting data in Landing folder.\n",
    "#         destination_path  = ingest_data_from_s3(currect_working_directory, s3, s3_path)\n",
    "#         logger.info(f\"ingestion of data Completed successfully at location {destination_path}\")\n",
    "\n",
    "\n",
    "#         spark = SparkSession.builder.master(\"local[*]\").appName(\"Batch_procecssing_pipeline_from_s3\").config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\").getOrCreate()\n",
    "#         logger.info(f\"SparkSession Created Successfully\")\n",
    "        \n",
    "#         logger.info(f\"apply_transformations function started successfully reading data from location : {destination_path}\")\n",
    "#         destination_path = apply_transformations(spark,destination_path)\n",
    "#         logger.info(f\"apply_transformations function completed saved parquet at location: {destination_path}\")\n",
    "#         df = generating_publisher_id_with_maximum_queries(spark, destination_path)\n",
    "#         logger.info(\"generating_publisher_id_with_maximum_queries function runned successfully\")\n",
    "#         df.coalesce(1).write.mode(\"overwrite\").csv(\"top_5_publishers_id_data\")\n",
    "#         logger.info(\"top-5 publishers_id saved in csv file\")\n",
    "#         generatring_line_graph_for_top_5_publishers(df, os.getcwd())\n",
    "#         logger.info(f\"generatring_line_graph_for_top_5_publishers function completed saved parquet at location: {destination_path}\")\n",
    "            \n",
    "# except Exception as e:\n",
    "#         logger.info(f\"Error has been encountered at main {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination_path = apply_transformations(spark,destination_path)\n",
    "# while True:\n",
    "#     print(\"yes\")\n",
    "\n",
    "logger.info(os.getcwd())\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Batch_procecssing_pipeline_from_s3\").config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "df = (spark.readStream.option(\"cleanSource\",\"archive\").option(\"sourceArchiveDir\", \"./archived/here/\").option(\"maxFilesPerTrigger\", 1).format(\"json\").load(f\"{os.getcwd()}/landing/\"))\n",
    "df.printSchema()\n",
    "df = df.select('*', \"ip_geo.*\", \"query.*\").drop(\"query\", \"ip_geo\")\n",
    "df = df.toDF(*get_unique_column_names(df.columns))\n",
    "\n",
    "df = df.drop(*get_duplicate_column_names(df))\n",
    "\n",
    "df = df.withColumn(\"real_filepath\", input_file_name())\n",
    "df.printSchema()\n",
    "df = df.withColumn(\"click_time\" ,col(\"click_time_1\").cast(\"timestamp\"))\n",
    "df.printSchema()\n",
    "# df.show()\n",
    "df = df.withColumn(\"actual_file\" , split(df.real_filepath, '/',limit=-1))\n",
    "df = df.withColumn(\"count_file\", size(df.actual_file))\n",
    "df = df.withColumn(\"actual_file\" , df.actual_file[col(\"count_file\")-1]).drop(\"count_file\")\n",
    "df = df.withColumn(\"file_creation_date\", get_file_generation_date_udf(col(\"actual_file\")))\n",
    "df = df.withColumn(\"file_creation_date\", to_timestamp(\"file_creation_date\", \"yyyy-MM-dd HH-mm\"))\n",
    "publisher_id  = get_publisher_id_column_name(df)\n",
    "df = df.na.fill(\"null\")\n",
    "# df.printSchema()\n",
    "# # print(\"this is the column structure\", df.columns)\n",
    "df = df.withColumnRenamed(publisher_id, \"publisher_id\")\n",
    "df = df.select(\"publisher_id\", \"file_creation_date\", \"actual_file\",\"click_time\")\n",
    "df = df.withColumn(\"publisher_id\", when(length(col(\"publisher_id\")) > 6, regexp_extract(col(\"publisher_id\"), \"^(va-\\d{3})|^(VA-\\d{3})\",0)).otherwise(col(\"publisher_id\")))\n",
    "\n",
    "# df.printSchema()\n",
    "df = df.withWatermark(\"click_time\", \"10 minutes\").groupBy(window(\"click_time\", \"10 minutes\"),\"publisher_id\", \"file_creation_date\", \"actual_file\").agg(count(\"publisher_id\").alias(\"total_clicks\"))\n",
    "# print(\"this is the window function count\", df.count())\n",
    "df = df.withColumn(\"date\", split(col(\"file_creation_date\"), \" \").getItem(0))\n",
    "df = df.withColumn(\"date\", to_timestamp(\"date\", \"yyyy-MM-dd\"))\n",
    "# df = df.withColumn(\"path\", lit(path))\n",
    "# df.printSchema()\n",
    "\n",
    "# df.write.partitionBy(\"date\").mode(\"append\").format(\"parquet\").save(str(os.getcwd()).replace(\"\\\\\", \"/\")+f'/clean1')\n",
    "df.writeStream.format(\"parquet\").option(\"path\", \"output/\").trigger(processingTime=\"30 seconds\").outputMode(\"append\").start().awaitTermination()\n",
    "# .option(\"path\", f\"{os.getcwd()}/output/\")\n",
    "# .option(\"path\", f\"{os.getcwd()}/output/\")\n",
    "# logger.info(f\"successfully saved data of {path} with partiton column date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- campaign_id: string (nullable = true)\n",
      " |-- click_time: long (nullable = true)\n",
      " |-- creative_id: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- gaid: string (nullable = true)\n",
      " |-- idfa: string (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- ip_geo: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- city_name: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- lat: string (nullable = true)\n",
      " |    |-- lon: string (nullable = true)\n",
      " |    |-- metro_code: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |    |-- zip: string (nullable = true)\n",
      " |-- isGlobalIpFlag: long (nullable = true)\n",
      " |-- p_click_id: string (nullable = true)\n",
      " |-- publisher_id: string (nullable = true)\n",
      " |-- query: struct (nullable = true)\n",
      " |    |-- _tk_: string (nullable = true)\n",
      " |    |-- campaign_id: string (nullable = true)\n",
      " |    |-- click_id: string (nullable = true)\n",
      " |    |-- creative_id: string (nullable = true)\n",
      " |    |-- gaid: string (nullable = true)\n",
      " |    |-- idfa: string (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- os_version: string (nullable = true)\n",
      " |    |-- publisher_id: string (nullable = true)\n",
      " |    |-- sub_publisher_id: string (nullable = true)\n",
      " |    |-- user_agent: string (nullable = true)\n",
      " |-- req: string (nullable = true)\n",
      " |-- sub_publisher_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Batch_procecssing_pipeline_from_s3\").config(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "df = (spark.readStream.option(\"cleanSource\",\"archive\").option(\"sourceArchiveDir\", \"./archived/here/\").option(\"maxFilesPerTrigger\", 1).format(\"json\").load(\"./landing\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717624170.5704577\n",
      "1717624170.5714736\n",
      "1.0159015655517578\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for i in range(0,31):\n",
    "    if i ==0:\n",
    "        start_time = time.time()\n",
    "        print(start_time)\n",
    "    elif i ==30:\n",
    "        end_time = time.time()\n",
    "        print(end_time)\n",
    "\n",
    "print((end_time-start_time)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717624309476514700\n"
     ]
    }
   ],
   "source": [
    "print(time.thread_time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
